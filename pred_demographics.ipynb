{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64984439",
   "metadata": {},
   "source": [
    "# Predict demographics of likely reconstructions\n",
    "\n",
    "We use the nearest neighbor target images to compare predictions and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81210d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # COLAB\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # LOCAL\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d078029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/reid-attack/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/reid-attack/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/reid-attack/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_82994/1180480125.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load('demographics/demographic_model.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load demographic classifier\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class DemographicModel(nn.Module):\n",
    "    def __init__(self, backbone=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pre-trained DenseNet121 from torchvision\n",
    "        if backbone is None:\n",
    "            base = models.densenet121(pretrained=True)\n",
    "            self.feature_extractor = nn.Sequential(*list(base.features.children()))\n",
    "            feature_dim = 1024 # final DenseNet121 feature size\n",
    "        else:\n",
    "            self.feature_extractor = backbone.features\n",
    "            feature_dim = backbone.features.denseblock4.denselayer16.norm2.num_features\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # task heads\n",
    "        self.age_head = nn.Linear(feature_dim, 1) # Regression for continuous age\n",
    "        self.sex_head = nn.Linear(feature_dim, 1) # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        age_pred = self.age_head(x).squeeze(1)\n",
    "        sex_prob = torch.sigmoid(self.sex_head(x)) # binary\n",
    "\n",
    "        return age_pred, sex_prob\n",
    "\n",
    "classifier = DemographicModel().eval().to(device)\n",
    "classifier.load_state_dict(torch.load('demographics/demographic_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72faa9",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13760130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_82994/3722155754.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load('recons/cnn/cnn_classes_cand2.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Load reconstructions\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samples = torch.load('recons/cnn/cnn_classes_cand2.pt', map_location='cpu')\n",
    "\n",
    "# Indices taken from overlap samples in maximization.ipynb\n",
    "overlap_indices = [np.int64(756), np.int64(881)]\n",
    "cnn_samples = samples[overlap_indices]\n",
    "\n",
    "# Resize - classifier expects 128x128\n",
    "cnn_samples = F.interpolate(cnn_samples, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "# To RGB for classifier\n",
    "cnn_samples = cnn_samples.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc26dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target images\n",
    "\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms \n",
    "import json \n",
    "import os\n",
    "\n",
    "# Same transforms as demographic classifier\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)), # Classifier expects 3-channels\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def load_target_images(target_paths):\n",
    "    target_imgs = []\n",
    "    for path in target_paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = transform(img)\n",
    "        target_imgs.append(img)\n",
    "    return torch.stack(target_imgs)\n",
    "\n",
    "# Target paths\n",
    "image_root = 'data/CheXpert_Sample'\n",
    "with open(\"target_models/images/cnn_images.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "target_paths = [os.path.join(image_root, entry[\"Path\"]) for entry in data]\n",
    "\n",
    "target_images = load_target_images(target_paths)\n",
    "# Indices taken from nn_indices in maximization.ipynb\n",
    "nn_indices = [1159, 1510]\n",
    "target_images = target_images[nn_indices]\n",
    "\n",
    "age = [entry[\"Age\"] for entry in data]\n",
    "sex = [entry[\"Sex\"] for entry in data]\n",
    "\n",
    "age_gt = [age[i] for i in nn_indices]\n",
    "sex_gt = [sex[i] for i in nn_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "755b2841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReconIndex</th>\n",
       "      <th>NNIndex</th>\n",
       "      <th>PredAge_Recon</th>\n",
       "      <th>PredAge_NN</th>\n",
       "      <th>Age_GT</th>\n",
       "      <th>AbsolutError_Recon</th>\n",
       "      <th>AbsoluteError_NN</th>\n",
       "      <th>PredSex_Recon</th>\n",
       "      <th>PredSex_NN</th>\n",
       "      <th>Sex_GT</th>\n",
       "      <th>Correct_Recon</th>\n",
       "      <th>Correct_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>756</td>\n",
       "      <td>1159</td>\n",
       "      <td>53.774960</td>\n",
       "      <td>27.855684</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.774960</td>\n",
       "      <td>6.855684</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>881</td>\n",
       "      <td>1510</td>\n",
       "      <td>56.650669</td>\n",
       "      <td>64.734993</td>\n",
       "      <td>39.0</td>\n",
       "      <td>17.650669</td>\n",
       "      <td>25.734993</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReconIndex  NNIndex  PredAge_Recon  PredAge_NN  Age_GT  AbsolutError_Recon  \\\n",
       "0         756     1159      53.774960   27.855684    21.0           32.774960   \n",
       "1         881     1510      56.650669   64.734993    39.0           17.650669   \n",
       "\n",
       "   AbsoluteError_NN PredSex_Recon PredSex_NN Sex_GT  Correct_Recon  Correct_NN  \n",
       "0          6.855684        Female       Male   Male          False        True  \n",
       "1         25.734993        Female       Male   Male          False        True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cnn_samples = cnn_samples.to(device)\n",
    "target_images = target_images.to(device)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Reconstructions\n",
    "    pred_age_recon, sex_logits_recon = classifier(cnn_samples)\n",
    "    sex_probs_recon = torch.sigmoid(sex_logits_recon.view(-1))\n",
    "    sex_preds_recon = (sex_probs_recon > 0.65).long()\n",
    "\n",
    "    # Nearest neighbor matches\n",
    "    pred_age_nn, sex_logits_nn = classifier(target_images)\n",
    "    sex_probs_nn = torch.sigmoid(sex_logits_nn.view(-1))\n",
    "    sex_preds_nn = (sex_probs_nn > 0.65).long()\n",
    "\n",
    "# Build dataframe\n",
    "results = []\n",
    "\n",
    "for i in range(len(cnn_samples)):\n",
    "    pred_age_r = pred_age_recon[i].item()\n",
    "    pred_sex_r = \"Male\" if sex_preds_recon[i].item() == 1 else \"Female\"\n",
    "    sex_conf_r = float(sex_probs_recon[i].item())\n",
    "\n",
    "    pred_age_nnm = pred_age_nn[i].item()\n",
    "    pred_sex_nnm = \"Male\" if sex_preds_nn[i].item() == 1 else \"Female\"\n",
    "    sex_conf_nnm = float(sex_probs_nn[i].item())\n",
    "\n",
    "    gt_age = age_gt[i]\n",
    "    gt_sex = sex_gt[i]\n",
    "\n",
    "    results.append({\n",
    "        # Indices\n",
    "        \"ReconIndex\": overlap_indices[i],\n",
    "        \"NNIndex\": nn_indices[i],\n",
    "\n",
    "        # Age\n",
    "        \"PredAge_Recon\": pred_age_r,\n",
    "        \"PredAge_NN\": pred_age_nnm,\n",
    "        \"Age_GT\": gt_age,\n",
    "        \"AbsolutError_Recon\": abs(pred_age_r - gt_age),\n",
    "        \"AbsoluteError_NN\": abs(pred_age_nnm - gt_age),\n",
    "\n",
    "        # Sex\n",
    "        \"PredSex_Recon\": pred_sex_r,\n",
    "        \"PredSex_NN\": pred_sex_nnm,\n",
    "        \"Sex_GT\": gt_sex,\n",
    "        \"Correct_Recon\": True if pred_sex_r == gt_sex else False,\n",
    "        \"Correct_NN\": True if pred_sex_nnm == gt_sex else False\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde3098",
   "metadata": {},
   "source": [
    "## Overfit CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29a7cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_82994/1251387932.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load('recons/cnn_overfit/cnn_overfit_cand.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Load reconstructions\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samples = torch.load('recons/cnn_overfit/cnn_overfit_cand.pt', map_location='cpu')\n",
    "\n",
    "# Indices taken from overlap samples in maximization.ipynb\n",
    "overlap_indices = [np.int64(54), np.int64(80), np.int64(203), np.int64(264), np.int64(307), np.int64(308), np.int64(319), np.int64(327), np.int64(371), np.int64(401), np.int64(418), np.int64(428), np.int64(445), np.int64(468), np.int64(472), np.int64(513), np.int64(548), np.int64(558), np.int64(568), np.int64(632), np.int64(633), np.int64(645), np.int64(767)]\n",
    "cnn_overfit_samples = samples[overlap_indices]\n",
    "\n",
    "# Resize - classifier expects 128x128\n",
    "cnn_overfit_samples = F.interpolate(cnn_overfit_samples, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "# To RGB for classifier\n",
    "# cnn_overfit_samples = cnn_overfit_samples.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77378dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target images\n",
    "\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms \n",
    "import json \n",
    "import os\n",
    "\n",
    "# Same transforms as demographic classifier\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)), # Classifier expects 3-channels\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def load_target_images(target_paths):\n",
    "    target_imgs = []\n",
    "    for path in target_paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = transform(img)\n",
    "        target_imgs.append(img)\n",
    "    return torch.stack(target_imgs)\n",
    "\n",
    "# Target paths\n",
    "image_root = 'data/CheXpert_Sample'\n",
    "with open(\"target_models/images/cnn_overfit_images.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "target_paths = [os.path.join(image_root, entry[\"Path\"]) for entry in data]\n",
    "\n",
    "target_images = load_target_images(target_paths)\n",
    "# Indices taken from nn_indices in maximization.ipynb\n",
    "nn_indices = [2, 2, 2, 2, 0, 0, 34, 15, 5, 2, 0, 2, 4, 0, 2, 31, 16, 17, 0, 17, 17, 2, 2]\n",
    "target_images = target_images[nn_indices]\n",
    "\n",
    "age = [entry[\"Age\"] for entry in data]\n",
    "sex = [entry[\"Sex\"] for entry in data]\n",
    "\n",
    "age_gt = [age[i] for i in nn_indices]\n",
    "sex_gt = [sex[i] for i in nn_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3478b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReconIndex</th>\n",
       "      <th>NNIndex</th>\n",
       "      <th>PredAge_Recon</th>\n",
       "      <th>PredAge_NN</th>\n",
       "      <th>Age_GT</th>\n",
       "      <th>AbsolutError_Recon</th>\n",
       "      <th>AbsoluteError_NN</th>\n",
       "      <th>PredSex_Recon</th>\n",
       "      <th>PredSex_NN</th>\n",
       "      <th>Sex_GT</th>\n",
       "      <th>Correct_Recon</th>\n",
       "      <th>Correct_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>51.082714</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.082714</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>40.073845</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.073845</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>2</td>\n",
       "      <td>57.408421</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.408421</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264</td>\n",
       "      <td>2</td>\n",
       "      <td>51.035839</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.035839</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>66.833267</td>\n",
       "      <td>38.156395</td>\n",
       "      <td>36.0</td>\n",
       "      <td>30.833267</td>\n",
       "      <td>2.156395</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>56.344059</td>\n",
       "      <td>38.156395</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.344059</td>\n",
       "      <td>2.156395</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>319</td>\n",
       "      <td>34</td>\n",
       "      <td>42.078274</td>\n",
       "      <td>54.446159</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.921726</td>\n",
       "      <td>5.553841</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>327</td>\n",
       "      <td>15</td>\n",
       "      <td>51.956535</td>\n",
       "      <td>48.091793</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.956535</td>\n",
       "      <td>0.908207</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>371</td>\n",
       "      <td>5</td>\n",
       "      <td>61.132042</td>\n",
       "      <td>43.359814</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.132042</td>\n",
       "      <td>12.359814</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>401</td>\n",
       "      <td>2</td>\n",
       "      <td>61.882851</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>23.882851</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>56.758392</td>\n",
       "      <td>38.156395</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.758392</td>\n",
       "      <td>2.156395</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>428</td>\n",
       "      <td>2</td>\n",
       "      <td>56.860374</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>18.860374</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>445</td>\n",
       "      <td>4</td>\n",
       "      <td>47.131290</td>\n",
       "      <td>37.414730</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.868710</td>\n",
       "      <td>10.585270</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>61.875927</td>\n",
       "      <td>38.156395</td>\n",
       "      <td>36.0</td>\n",
       "      <td>25.875927</td>\n",
       "      <td>2.156395</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>472</td>\n",
       "      <td>2</td>\n",
       "      <td>54.889587</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.889587</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>513</td>\n",
       "      <td>31</td>\n",
       "      <td>49.711559</td>\n",
       "      <td>44.659725</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.711559</td>\n",
       "      <td>12.659725</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>548</td>\n",
       "      <td>16</td>\n",
       "      <td>49.670216</td>\n",
       "      <td>22.176233</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.670216</td>\n",
       "      <td>5.823767</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>558</td>\n",
       "      <td>17</td>\n",
       "      <td>63.450840</td>\n",
       "      <td>55.217884</td>\n",
       "      <td>44.0</td>\n",
       "      <td>19.450840</td>\n",
       "      <td>11.217884</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>38.040051</td>\n",
       "      <td>38.156395</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.040051</td>\n",
       "      <td>2.156395</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>632</td>\n",
       "      <td>17</td>\n",
       "      <td>42.855961</td>\n",
       "      <td>55.217884</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.144039</td>\n",
       "      <td>11.217884</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>633</td>\n",
       "      <td>17</td>\n",
       "      <td>65.285606</td>\n",
       "      <td>55.217884</td>\n",
       "      <td>44.0</td>\n",
       "      <td>21.285606</td>\n",
       "      <td>11.217884</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>645</td>\n",
       "      <td>2</td>\n",
       "      <td>45.752834</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.752834</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>767</td>\n",
       "      <td>2</td>\n",
       "      <td>67.026489</td>\n",
       "      <td>36.347134</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.026489</td>\n",
       "      <td>1.652866</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ReconIndex  NNIndex  PredAge_Recon  PredAge_NN  Age_GT  \\\n",
       "0           54        2      51.082714   36.347134    38.0   \n",
       "1           80        2      40.073845   36.347134    38.0   \n",
       "2          203        2      57.408421   36.347134    38.0   \n",
       "3          264        2      51.035839   36.347134    38.0   \n",
       "4          307        0      66.833267   38.156395    36.0   \n",
       "5          308        0      56.344059   38.156395    36.0   \n",
       "6          319       34      42.078274   54.446159    60.0   \n",
       "7          327       15      51.956535   48.091793    49.0   \n",
       "8          371        5      61.132042   43.359814    31.0   \n",
       "9          401        2      61.882851   36.347134    38.0   \n",
       "10         418        0      56.758392   38.156395    36.0   \n",
       "11         428        2      56.860374   36.347134    38.0   \n",
       "12         445        4      47.131290   37.414730    48.0   \n",
       "13         468        0      61.875927   38.156395    36.0   \n",
       "14         472        2      54.889587   36.347134    38.0   \n",
       "15         513       31      49.711559   44.659725    32.0   \n",
       "16         548       16      49.670216   22.176233    28.0   \n",
       "17         558       17      63.450840   55.217884    44.0   \n",
       "18         568        0      38.040051   38.156395    36.0   \n",
       "19         632       17      42.855961   55.217884    44.0   \n",
       "20         633       17      65.285606   55.217884    44.0   \n",
       "21         645        2      45.752834   36.347134    38.0   \n",
       "22         767        2      67.026489   36.347134    38.0   \n",
       "\n",
       "    AbsolutError_Recon  AbsoluteError_NN PredSex_Recon PredSex_NN  Sex_GT  \\\n",
       "0            13.082714          1.652866          Male       Male    Male   \n",
       "1             2.073845          1.652866          Male       Male    Male   \n",
       "2            19.408421          1.652866          Male       Male    Male   \n",
       "3            13.035839          1.652866          Male       Male    Male   \n",
       "4            30.833267          2.156395        Female     Female  Female   \n",
       "5            20.344059          2.156395          Male     Female  Female   \n",
       "6            17.921726          5.553841          Male       Male    Male   \n",
       "7             2.956535          0.908207          Male     Female  Female   \n",
       "8            30.132042         12.359814          Male     Female  Female   \n",
       "9            23.882851          1.652866          Male       Male    Male   \n",
       "10           20.758392          2.156395          Male     Female  Female   \n",
       "11           18.860374          1.652866          Male       Male    Male   \n",
       "12            0.868710         10.585270        Female       Male    Male   \n",
       "13           25.875927          2.156395        Female     Female  Female   \n",
       "14           16.889587          1.652866        Female       Male    Male   \n",
       "15           17.711559         12.659725        Female       Male    Male   \n",
       "16           21.670216          5.823767          Male     Female  Female   \n",
       "17           19.450840         11.217884          Male       Male    Male   \n",
       "18            2.040051          2.156395        Female     Female  Female   \n",
       "19            1.144039         11.217884          Male       Male    Male   \n",
       "20           21.285606         11.217884          Male       Male    Male   \n",
       "21            7.752834          1.652866          Male       Male    Male   \n",
       "22           29.026489          1.652866          Male       Male    Male   \n",
       "\n",
       "    Correct_Recon  Correct_NN  \n",
       "0            True        True  \n",
       "1            True        True  \n",
       "2            True        True  \n",
       "3            True        True  \n",
       "4            True        True  \n",
       "5           False        True  \n",
       "6            True        True  \n",
       "7           False        True  \n",
       "8           False        True  \n",
       "9            True        True  \n",
       "10          False        True  \n",
       "11           True        True  \n",
       "12          False        True  \n",
       "13           True        True  \n",
       "14          False        True  \n",
       "15          False        True  \n",
       "16          False        True  \n",
       "17           True        True  \n",
       "18           True        True  \n",
       "19           True        True  \n",
       "20           True        True  \n",
       "21           True        True  \n",
       "22           True        True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cnn_overfit_samples = cnn_overfit_samples.to(device)\n",
    "target_images = target_images.to(device)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Reconstructions\n",
    "    pred_age_recon, sex_logits_recon = classifier(cnn_overfit_samples)\n",
    "    sex_probs_recon = torch.sigmoid(sex_logits_recon.view(-1))\n",
    "    sex_preds_recon = (sex_probs_recon > 0.65).long()\n",
    "\n",
    "    # Nearest neighbor matches\n",
    "    pred_age_nn, sex_logits_nn = classifier(target_images)\n",
    "    sex_probs_nn = torch.sigmoid(sex_logits_nn.view(-1))\n",
    "    sex_preds_nn = (sex_probs_nn > 0.65).long()\n",
    "\n",
    "# Build dataframe\n",
    "results = []\n",
    "\n",
    "for i in range(len(cnn_overfit_samples)):\n",
    "    pred_age_r = pred_age_recon[i].item()\n",
    "    pred_sex_r = \"Male\" if sex_preds_recon[i].item() == 1 else \"Female\"\n",
    "    sex_conf_r = float(sex_probs_recon[i].item())\n",
    "\n",
    "    pred_age_nnm = pred_age_nn[i].item()\n",
    "    pred_sex_nnm = \"Male\" if sex_preds_nn[i].item() == 1 else \"Female\"\n",
    "    sex_conf_nnm = float(sex_probs_nn[i].item())\n",
    "\n",
    "    gt_age = age_gt[i]\n",
    "    gt_sex = sex_gt[i]\n",
    "\n",
    "    results.append({\n",
    "        # Indices\n",
    "        \"ReconIndex\": overlap_indices[i],\n",
    "        \"NNIndex\": nn_indices[i],\n",
    "\n",
    "        # Age\n",
    "        \"PredAge_Recon\": pred_age_r,\n",
    "        \"PredAge_NN\": pred_age_nnm,\n",
    "        \"Age_GT\": gt_age,\n",
    "        \"AbsolutError_Recon\": abs(pred_age_r - gt_age),\n",
    "        \"AbsoluteError_NN\": abs(pred_age_nnm - gt_age),\n",
    "\n",
    "        # Sex\n",
    "        \"PredSex_Recon\": pred_sex_r,\n",
    "        \"PredSex_NN\": pred_sex_nnm,\n",
    "        \"Sex_GT\": gt_sex,\n",
    "        \"Correct_Recon\": True if pred_sex_r == gt_sex else False,\n",
    "        \"Correct_NN\": True if pred_sex_nnm == gt_sex else False\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943984e",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aef584aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_81679/4060129642.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load('recons/vit/vit_cand.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Load reconstructions\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samples = torch.load('recons/vit/vit_cand.pt', map_location='cpu')\n",
    "\n",
    "# Indices taken from overlap samples in maximization.ipynb\n",
    "overlap_indices = [np.int64(548)]\n",
    "vit_samples = samples[overlap_indices]\n",
    "\n",
    "# Resize - classifier expects 128x128\n",
    "vit_samples = F.interpolate(vit_samples, size=(128, 128), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dca02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target paths\n",
    "image_root = 'data/CheXpert_Sample'\n",
    "with open(\"target_models/images/vit_images.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "target_paths = [os.path.join(image_root, entry[\"Path\"]) for entry in data]\n",
    "\n",
    "target_images = load_target_images(target_paths)\n",
    "# Indices taken from nn_indices in maximization.ipynb\n",
    "nn_indices = [857]\n",
    "target_images = target_images[nn_indices]\n",
    "\n",
    "age = [entry[\"Age\"] for entry in data]\n",
    "sex = [entry[\"Sex\"] for entry in data]\n",
    "\n",
    "age_gt = [age[i] for i in nn_indices]\n",
    "sex_gt = [sex[i] for i in nn_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8895ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReconIndex</th>\n",
       "      <th>NNIndex</th>\n",
       "      <th>PredAge_Recon</th>\n",
       "      <th>PredAge_NN</th>\n",
       "      <th>Age_GT</th>\n",
       "      <th>AbsolutError_Recon</th>\n",
       "      <th>AbsoluteError_NN</th>\n",
       "      <th>PredSex_Recon</th>\n",
       "      <th>PredSex_NN</th>\n",
       "      <th>Sex_GT</th>\n",
       "      <th>Correct_Recon</th>\n",
       "      <th>Correct_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>548</td>\n",
       "      <td>857</td>\n",
       "      <td>45.281887</td>\n",
       "      <td>73.384995</td>\n",
       "      <td>69.0</td>\n",
       "      <td>23.718113</td>\n",
       "      <td>4.384995</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReconIndex  NNIndex  PredAge_Recon  PredAge_NN  Age_GT  AbsolutError_Recon  \\\n",
       "0         548      857      45.281887   73.384995    69.0           23.718113   \n",
       "\n",
       "   AbsoluteError_NN PredSex_Recon PredSex_NN Sex_GT  Correct_Recon  Correct_NN  \n",
       "0          4.384995          Male       Male   Male           True        True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vit_samples = vit_samples.to(device)\n",
    "target_images = target_images.to(device)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Reconstructions\n",
    "    pred_age_recon, sex_logits_recon = classifier(vit_samples)\n",
    "    sex_probs_recon = torch.sigmoid(sex_logits_recon.view(-1))\n",
    "    sex_preds_recon = (sex_probs_recon > 0.65).long()\n",
    "\n",
    "    # Nearest neighbor matches\n",
    "    pred_age_nn, sex_logits_nn = classifier(target_images)\n",
    "    sex_probs_nn = torch.sigmoid(sex_logits_nn.view(-1))\n",
    "    sex_preds_nn = (sex_probs_nn > 0.65).long()\n",
    "\n",
    "# Build dataframe\n",
    "results = []\n",
    "\n",
    "for i in range(len(vit_samples)):\n",
    "    pred_age_r = pred_age_recon[i].item()\n",
    "    pred_sex_r = \"Male\" if sex_preds_recon[i].item() == 1 else \"Female\"\n",
    "    sex_conf_r = float(sex_probs_recon[i].item())\n",
    "\n",
    "    pred_age_nnm = pred_age_nn[i].item()\n",
    "    pred_sex_nnm = \"Male\" if sex_preds_nn[i].item() == 1 else \"Female\"\n",
    "    sex_conf_nnm = float(sex_probs_nn[i].item())\n",
    "\n",
    "    gt_age = age_gt[i]\n",
    "    gt_sex = sex_gt[i]\n",
    "\n",
    "    results.append({\n",
    "        # Indices\n",
    "        \"ReconIndex\": overlap_indices[i],\n",
    "        \"NNIndex\": nn_indices[i],\n",
    "\n",
    "        # Age\n",
    "        \"PredAge_Recon\": pred_age_r,\n",
    "        \"PredAge_NN\": pred_age_nnm,\n",
    "        \"Age_GT\": gt_age,\n",
    "        \"AbsolutError_Recon\": abs(pred_age_r - gt_age),\n",
    "        \"AbsoluteError_NN\": abs(pred_age_nnm - gt_age),\n",
    "\n",
    "        # Sex\n",
    "        \"PredSex_Recon\": pred_sex_r,\n",
    "        \"PredSex_NN\": pred_sex_nnm,\n",
    "        \"Sex_GT\": gt_sex,\n",
    "        \"Correct_Recon\": True if pred_sex_r == gt_sex else False,\n",
    "        \"Correct_NN\": True if pred_sex_nnm == gt_sex else False\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a5302",
   "metadata": {},
   "source": [
    "## Overfit ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aca435f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_81679/881537187.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load('recons/vit_overfit/vit_overfit_cand.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Load reconstructions\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samples = torch.load('recons/vit_overfit/vit_overfit_cand.pt', map_location='cpu')\n",
    "\n",
    "# Indices taken from overlap samples in maximization.ipynb\n",
    "overlap_indices = [np.int64(33), np.int64(790)]\n",
    "vit_overfit_samples = samples[overlap_indices]\n",
    "\n",
    "# Resize - classifier expects 128x128\n",
    "vit_overfit_samples = F.interpolate(vit_overfit_samples, size=(128, 128), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63368004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target paths\n",
    "image_root = 'data/CheXpert_Sample'\n",
    "with open(\"target_models/images/vit_images.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "target_paths = [os.path.join(image_root, entry[\"Path\"]) for entry in data]\n",
    "\n",
    "target_images = load_target_images(target_paths)\n",
    "# Indices taken from nn_indices in maximization.ipynb\n",
    "nn_indices = [59, 8]\n",
    "target_images = target_images[nn_indices]\n",
    "\n",
    "age = [entry[\"Age\"] for entry in data]\n",
    "sex = [entry[\"Sex\"] for entry in data]\n",
    "\n",
    "age_gt = [age[i] for i in nn_indices]\n",
    "sex_gt = [sex[i] for i in nn_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28d7d07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReconIndex</th>\n",
       "      <th>NNIndex</th>\n",
       "      <th>PredAge_Recon</th>\n",
       "      <th>PredAge_NN</th>\n",
       "      <th>Age_GT</th>\n",
       "      <th>AbsolutError_Recon</th>\n",
       "      <th>AbsoluteError_NN</th>\n",
       "      <th>PredSex_Recon</th>\n",
       "      <th>PredSex_NN</th>\n",
       "      <th>Sex_GT</th>\n",
       "      <th>Correct_Recon</th>\n",
       "      <th>Correct_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>43.817287</td>\n",
       "      <td>30.172731</td>\n",
       "      <td>33.0</td>\n",
       "      <td>10.817287</td>\n",
       "      <td>2.827269</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>790</td>\n",
       "      <td>8</td>\n",
       "      <td>57.571026</td>\n",
       "      <td>60.201504</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.571026</td>\n",
       "      <td>6.201504</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReconIndex  NNIndex  PredAge_Recon  PredAge_NN  Age_GT  AbsolutError_Recon  \\\n",
       "0          33       59      43.817287   30.172731    33.0           10.817287   \n",
       "1         790        8      57.571026   60.201504    54.0            3.571026   \n",
       "\n",
       "   AbsoluteError_NN PredSex_Recon PredSex_NN  Sex_GT  Correct_Recon  \\\n",
       "0          2.827269        Female     Female  Female           True   \n",
       "1          6.201504          Male       Male    Male           True   \n",
       "\n",
       "   Correct_NN  \n",
       "0        True  \n",
       "1        True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vit_overfit_samples = vit_overfit_samples.to(device)\n",
    "target_images = target_images.to(device)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Reconstructions\n",
    "    pred_age_recon, sex_logits_recon = classifier(vit_overfit_samples)\n",
    "    sex_probs_recon = torch.sigmoid(sex_logits_recon.view(-1))\n",
    "    sex_preds_recon = (sex_probs_recon > 0.65).long()\n",
    "\n",
    "    # Nearest neighbor matches\n",
    "    pred_age_nn, sex_logits_nn = classifier(target_images)\n",
    "    sex_probs_nn = torch.sigmoid(sex_logits_nn.view(-1))\n",
    "    sex_preds_nn = (sex_probs_nn > 0.65).long()\n",
    "\n",
    "# Build dataframe\n",
    "results = []\n",
    "\n",
    "for i in range(len(vit_overfit_samples)):\n",
    "    pred_age_r = pred_age_recon[i].item()\n",
    "    pred_sex_r = \"Male\" if sex_preds_recon[i].item() == 1 else \"Female\"\n",
    "    sex_conf_r = float(sex_probs_recon[i].item())\n",
    "\n",
    "    pred_age_nnm = pred_age_nn[i].item()\n",
    "    pred_sex_nnm = \"Male\" if sex_preds_nn[i].item() == 1 else \"Female\"\n",
    "    sex_conf_nnm = float(sex_probs_nn[i].item())\n",
    "\n",
    "    gt_age = age_gt[i]\n",
    "    gt_sex = sex_gt[i]\n",
    "\n",
    "    results.append({\n",
    "        # Indices\n",
    "        \"ReconIndex\": overlap_indices[i],\n",
    "        \"NNIndex\": nn_indices[i],\n",
    "\n",
    "        # Age\n",
    "        \"PredAge_Recon\": pred_age_r,\n",
    "        \"PredAge_NN\": pred_age_nnm,\n",
    "        \"Age_GT\": gt_age,\n",
    "        \"AbsolutError_Recon\": abs(pred_age_r - gt_age),\n",
    "        \"AbsoluteError_NN\": abs(pred_age_nnm - gt_age),\n",
    "\n",
    "        # Sex\n",
    "        \"PredSex_Recon\": pred_sex_r,\n",
    "        \"PredSex_NN\": pred_sex_nnm,\n",
    "        \"Sex_GT\": gt_sex,\n",
    "        \"Correct_Recon\": True if pred_sex_r == gt_sex else False,\n",
    "        \"Correct_NN\": True if pred_sex_nnm == gt_sex else False\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecf5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid-attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
